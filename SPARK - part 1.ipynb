{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "list = [randint(10,1000) for x in range(0,20000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\BigDataLocalSetup\\\\spark'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "--- 0.001001596450805664 seconds ---\n",
      "16162554\n"
     ]
    }
   ],
   "source": [
    "from random import randint \n",
    "import time\n",
    "\n",
    "# create a list of random numbers between 10 to 1000\n",
    "my_large_list = [randint(10,1000) for x in range(0,20000000)]\n",
    "\n",
    "# create one partition of the list  \n",
    "my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=1)\n",
    "\n",
    "# check number of partitions\n",
    "print(my_large_list_one_partition.getNumPartitions())\n",
    "# >> 1\n",
    "start_time = time.time()\n",
    "# filter numbers greater than equal to 200\n",
    "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
    "\n",
    "# code was run in a jupyter notebook \n",
    "# to calculate the time taken to execute the following command\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# count the number of elements in filtered list\n",
    "print(my_large_list_one_partition.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "--- 0.0 seconds ---\n",
      "16164300\n"
     ]
    }
   ],
   "source": [
    "from random import randint \n",
    "import time\n",
    "\n",
    "# create a list of random numbers between 10 to 1000\n",
    "my_large_list = [randint(10,1000) for x in range(0,20000000)]\n",
    "\n",
    "# create one partition of the list  \n",
    "my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=5)\n",
    "\n",
    "# check number of partitions\n",
    "print(my_large_list_one_partition.getNumPartitions())\n",
    "# >> 1\n",
    "start_time = time.time()\n",
    "# filter numbers greater than equal to 200\n",
    "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
    "\n",
    "# code was run in a jupyter notebook \n",
    "# to calculate the time taken to execute the following command\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# count the number of elements in filtered list\n",
    "print(my_large_list_one_partition.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the above we can see that the time taken to count and filter the values less then 200 is 0 when we use 5 slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can able to see ther are two types of trasformation in spark\n",
    "> Narrow Transformation\n",
    "\n",
    "> Wide Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Narrow Trasformation all the elements required to compute the single partition lies on that single partition of parent RDD\n",
    "\n",
    "In Wide Transformation all the elements required to compute the single partition may live more than single partitions  the parent RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lazy Evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark  has special characteristic called lazy evaluation, where if we need to do a transformation like mapping, filtering, spliting letters on data. it will transform the data bassed on the results that we need thus reducing the unnecessary computaion\n",
    "\n",
    "Eg:\n",
    "\n",
    "if we have data of 1GB of 10 partition and we need to transform the data set to lower case. but we want to see the first line of the data alone. In this case spark will read the file from only first partition and perform the trasformation to give out the results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data = [i for i in range(10,200000)]\n",
    "sp_data = sc.parallelize(list_data,numSlices=5)\n",
    "sp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adding 4 to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[17] at RDD at PythonRDD.scala:53\n",
      "b'(5) PythonRDD[17] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:274 []'\n"
     ]
    }
   ],
   "source": [
    "sp_data1 = sp_data.map(lambda x: x+4)\n",
    "print(sp_data1)\n",
    "print(sp_data1.toDebugString())\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Again adding 20 to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[18] at RDD at PythonRDD.scala:53\n",
      "b'(5) PythonRDD[17] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:274 []'\n"
     ]
    }
   ],
   "source": [
    "sp_data2 = sp_data1.map(lambda x: x+20)\n",
    "print(sp_data2)\n",
    "print(sp_data1.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it will add 24 directly insted of adding 4 and then 20, thus spark can able to define the path by itself easily "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLlib and Local Vectors\n",
    "\n",
    "Spark uses MLlib to perform machine learning algorithams like linear regression, logistic, dimensional reduction and other statistical calculations.\n",
    "\n",
    "\n",
    "##### Local vectors\n",
    "\n",
    "MLlib uses two vectors dense and sparse vectors\n",
    "\n",
    "dense vectors doesnt have any index for the specific values, sparse has index for the values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "print(Vectors.dense([1,2,3,4,5,6,7,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,[0,1,2,4,5],[1.0,5.0,3.0,5.0,7.0])\n"
     ]
    }
   ],
   "source": [
    "print(Vectors.sparse(10, [0,1,2,4,5], [1.0,5.0,3.0,5.0,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 5. 3. 0. 5. 7. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Vectors.sparse(10, [0,1,2,4,5], [1.0,5.0,3.0,5.0,7]).toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labled Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0,4.0,5.0,6.0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "point = LabeledPoint(1,Vectors.dense([1,2,3,4,5,6]))\n",
    "print(point.features)\n",
    "print(point.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local matrix are stored in single machine it supports bith sparse and dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9. 6. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 8.]]\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "mat1 = Matrices.dense(3,2,[1,2,3,4,5,6])\n",
    "mat2 = Matrices.sparse(3, 3, [0, 1, 2, 3], [0, 0, 2], [9, 6, 8])\n",
    "print(mat2.toArray())\n",
    "print(mat1.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Matrices\n",
    "\n",
    "Distributed matrices are stored in one or more RDDs. FOur types of distributed matrices are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Row Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a local vector, we can store rows in multiple partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithams like random forest can be executed by this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x000001D1E89E8C70>\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# create RDD\n",
    "rows = sc.parallelize([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])\n",
    "\n",
    "# create a distributed Row Matrix\n",
    "row_matrix = RowMatrix(rows)\n",
    "\n",
    "\n",
    "print(row_matrix)\n",
    "# >> <pyspark.mllib.linalg.distributed.RowMatrix at 0x7f425884d7f0> \n",
    "\n",
    "print(row_matrix.numRows())\n",
    "# >> 4\n",
    "\n",
    "print(row_matrix.numCols())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexed Row Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the rows are stored similar to row matrices in each partiotion but the rows are indexed in an ordered way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "# create RDD\n",
    "indexed_rows = sc.parallelize([\n",
    "    IndexedRow(0, [0,1,2]),\n",
    "    IndexedRow(1, [1,2,3]),\n",
    "    IndexedRow(2, [3,4,5]),\n",
    "    IndexedRow(3, [4,2,3]),\n",
    "    IndexedRow(4, [2,2,5]),\n",
    "    IndexedRow(5, [4,5,5])\n",
    "])\n",
    "\n",
    "# create IndexedRowMatrix\n",
    "indexed_rows_matrix = IndexedRowMatrix(indexed_rows)\n",
    "\n",
    "print(indexed_rows_matrix.numRows())\n",
    "# >> 6\n",
    "\n",
    "print(indexed_rows_matrix.numCols())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Coordinate Matrices\n",
    "\n",
    "We use this matrices when dimensions of the matrix are very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "# Create an RDD of coordinate entries with the MatrixEntry class:\n",
    "matrix_entries = sc.parallelize([MatrixEntry(0, 5, 2), MatrixEntry(1, 1, 1), MatrixEntry(1, 5, 4)])\n",
    "\n",
    "# Create an CoordinateMatrix from an RDD of MatrixEntries.\n",
    "c_matrix = CoordinateMatrix(matrix_entries)\n",
    "\n",
    "# number of columns\n",
    "print(c_matrix.numCols())\n",
    "# >> 6\n",
    "\n",
    "# number of rows\n",
    "print(c_matrix.numRows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Block Matrix\n",
    "\n",
    "In this matrices the sub matrices are stored inside  the matrices\n",
    "\n",
    "we need to specify the dimetion of the matrices along with a inside sub matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "[[1. 2. 1. 0. 0. 0.]\n",
      " [2. 1. 2. 0. 0. 0.]\n",
      " [1. 2. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 3. 3. 3.]\n",
      " [0. 0. 0. 4. 4. 4.]\n",
      " [0. 0. 0. 5. 5. 5.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 3, [1, 2, 1, 2, 1, 2, 1, 2, 1])),\n",
    "                         ((1, 1), Matrices.dense(3, 3, [3, 4, 5, 3, 4, 5, 3, 4, 5])),\n",
    "                         ((2, 0), Matrices.dense(3, 3, [1, 1, 1, 1, 1, 1, 1, 1, 1]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks  of size 3X3\n",
    "b_matrix = BlockMatrix(blocks, 3, 3) \n",
    "\n",
    "# columns per block\n",
    "print(b_matrix.colsPerBlock)\n",
    "# >> 3\n",
    "\n",
    "# rows per block\n",
    "print(b_matrix.rowsPerBlock)\n",
    "# >> 3\n",
    "\n",
    "# convert the block matrix to local matrix\n",
    "local_mat = b_matrix.toLocalMatrix()\n",
    "\n",
    "# print local matrix\n",
    "print(local_mat.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
